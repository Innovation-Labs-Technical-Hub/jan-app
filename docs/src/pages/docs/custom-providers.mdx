---
title: Adding New Providers
description: Learn about how to add new providers to Jan.
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    Jan Extensions,
    Extensions,
  ]
---

import { Callout } from 'nextra/components'
import { Settings, EllipsisVertical } from 'lucide-react'

# Adding New Providers

Any local and remote model provider that has a compatible OpenAI API can be used in combination with Jan and both
can be added in the same way.

## Local

Jan only allows you to run GGUF files but you can add other files via an OpenAI API compatible server like [vllm](https://vllm.ai/)
or [SGLang]().

### vLLM

First, make sure you have [uv](https://docs.astral.sh/uv/) installed.

Next, create a virtual environment to install vLLM in it.

```sh
uv venv .venv --python 3.12
```

Activate your environment.
```sh
source .venv/bin/activate
```

Install `vllm` in it.
```sh
uv pip install vllm
```

Next, start a server and download a model at the same time.




### SGLang

Create a virtual environment to install vLLM in it.

```sh
uv venv .venv --python 3.12
```

Activate your environment.
```sh
source .venv/bin/activate
```

Install `vllm` in it.
```sh
uv pip install vllm
```

Next, start a server and download a model at the same time.


## Install Remote Engines

You can add any OpenAI API-compatible providers like Together AI, Fireworks AI, and others. Let's walk through
some examples.

### Together AI

Create an account or login to existing [Together AI](https://together.ai)'s Dashboard.

![Together's Dashboard](./_assets/together.png)

Click on a View all models.


Pick a free model like `deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free`.


Where it says **Run Inference**, click on `curl` and grab:

- `url`: `https://api.together.xyz/v1/chat/completions`
- `model`: `deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free`

Get your API key.


Go back to Jan and open **Settings > Model Providers** and click on **Add Provider**.



Name it Together and proceed.


Add your API key and change the URL.


Enter the `model id` by clicking on the `+` sign where it says Models.




1. Navigate to **Settings** (<Settings width={16} height={16} style={{display:"inline"}}/>) > **Engines**
1. At **Remote Engine** category, click **+ Install Engine**

<br/>
![Install Remote Engines](./_assets/install-engines-01.png)
<br/>

2. Fill in the following required information:

<br/>
![Install Remote Engines](./_assets/install-engines-02.png)
<br/>

| Field | Description | Required |
|-------|-------------|----------|
| Engine Name | Name for your engine (e.g., "OpenAI", "Claude") | ✓ |
| API URL | The base URL of the provider's API | ✓ |
| API Key | Your authentication key to activate this engine | ✓ |
| Model List URL | The endpoint URL to fetch available models |
| API Key Template | Custom authorization header format | |
| Request Format Conversion | Function to convert Jan's request format to provider's format | |
| Response Format Conversion | Function to convert provider's response format to Jan's format | |


> - The conversion functions are only needed for providers that don't follow the OpenAI API format. For OpenAI-compatible APIs, you can leave these empty.
> - For OpenAI-compatible APIs like OpenAI, Anthropic, or Groq, you only need to fill in the required fields. Leave optional fields empty.

4. Click **Install**
5. Once completed, you should see your engine in **Engines** page:
    - You can rename or uninstall your engine
    - You can navigate to its own settings page

<br/>
![Install Remote Engines](./_assets/install-engines-03.png)
<br/>

### Examples
#### OpenAI-Compatible Setup
Here's how to set up OpenAI as a remote engine:

1. Engine Name: `OpenAI`
2. API URL: `https://api.openai.com`
3. Model List URL: `https://api.openai.com/v1/models`
4. API Key: Your OpenAI API key
5. Leave other fields as default


#### Custom APIs Setup
If you're integrating an API that doesn't follow OpenAI's format, you'll need to use the conversion functions.
Let's say you have a custom API with this format:

```javascript
// Custom API Request Format
{
  "prompt": "What is AI?",
  "max_length": 100,
  "temperature": 0.7
}

// Custom API Response Format
{
  "generated_text": "AI is...",
  "tokens_used": 50,
  "status": "success"
}
```

**Here's how to set it up in Jan:**
```
Engine Name: Custom LLM
API Key: your_api_key_here
Header template: your header template here
Transform request template: your transform request template here
Transform response template: your transform response template here
```

1. Header template
```json
"Authorization: Bearer {{api_key}}"
```
2. Transform request template:
Convert from Jan's OpenAI-style format to your API's format

```json
"chat_completions": {
  "url": "https://api.custom_endpoint.com/v1/messages",
  "template": "{
{% for key, value in input_request %}
  {% if key == "messages" %}
    "prompt": "{{ last(input_request.messages).content }}"
  {% else if key == "max_tokens" or key == "temperature" %}
    "{{ key }}": {{ tojson(value) }}
  {% endif %}
{% endfor %}
}"
}
```

3. Transform response template
Convert from your API's format back to OpenAI-style format

```json
"chat_completions": {
  "template": {
    "choices": [{
        "message": {
          "role": "assistant",
          "content": "{{ input_request.generated_text }}"
        }
      }],
      "usage": {
        "total_tokens": "{{ input_request.tokens_used }}"
      }
  }
}
```

**Expected Formats:**

1. Jan's Request Format

```json
{
  "messages": [
    {"role": "user", "content": "What is AI?"}
  ],
  "max_tokens": 100,
  "temperature": 0.7
}
```

2. Jan's Expected Response Format

```json
{
  "choices": [{
    "message": {
      "role": "assistant",
      "content": "AI is..."
    }
  }],
  "usage": {
    "total_tokens": 50
  }
}
```
